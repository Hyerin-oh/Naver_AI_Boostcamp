{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compatible-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torchvision.models as models \n",
    "import torchvision.models._utils as _utils\n",
    "\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpine-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter \n",
    "img_path = 'input/data/train/images'\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cubic-jacob",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>mask</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>000001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>000001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>000001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>000001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>000001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path      id  mask  gender  \\\n",
       "0  input/data/train/images/000001_female_Asian_45...  000001     0       1   \n",
       "1  input/data/train/images/000001_female_Asian_45...  000001     2       1   \n",
       "2  input/data/train/images/000001_female_Asian_45...  000001     0       1   \n",
       "3  input/data/train/images/000001_female_Asian_45...  000001     0       1   \n",
       "4  input/data/train/images/000001_female_Asian_45...  000001     1       1   \n",
       "\n",
       "   age  label  \n",
       "0    1      4  \n",
       "1    1     16  \n",
       "2    1      4  \n",
       "3    1      4  \n",
       "4    1     10  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessing_data.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compressed-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path , df ,transform = None):\n",
    "        \n",
    "        self.path = path\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        image = cv2.imread(self.df['path'].iloc[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image) \n",
    "            image = augmented['image']\n",
    "        # label = self.df['label'].iloc[idx]\n",
    "        age =  self.df['age'].iloc[idx]\n",
    "        gender =  self.df['gender'].iloc[idx]\n",
    "        mask =  self.df['mask'].iloc[idx]\n",
    "        \n",
    "        labels = (age ,  gender , mask)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "timely-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple, Sequence, Optional\n",
    "        \n",
    "def get_center_crop_coords(img , crop_height: int, crop_width: int, ratio : int):\n",
    "    transformed_img = img.copy()\n",
    "    height = img.shape[1]\n",
    "    width = img.shape[0]\n",
    "    delta = random.uniform(-ratio,ratio)\n",
    "    y1 = int(np.round((1 + delta) * (height - crop_height) // 2))\n",
    "    y2 = y1 + crop_height\n",
    "    x1 = int(np.round((1 + delta) * (height - crop_width) // 2))\n",
    "    x2 = x1 + crop_width\n",
    "    return transformed_img[y1:y2 , x1:x2]\n",
    "\n",
    "class CustomRandomCrop(ImageOnlyTransform):\n",
    "    def __init__(self, height, width, ratio, always_apply=False, p=1.0):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.crop_height = height\n",
    "        self.crop_width = width\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def apply(self, img, **params):\n",
    "        return get_center_crop_coords(img,self.crop_height, self.crop_width , self.ratio )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stuck-centre",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# albumentation 사용 \n",
    "\n",
    "mean = [0.560 , 0.524 , 0.501]\n",
    "std = [0.233 , 0.243 , 0.246]\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    CustomRandomCrop(224,224,0.1),\n",
    "    A.Normalize(mean , std),\n",
    "    A.pytorch.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    CustomRandomCrop(224,224,0.1),\n",
    "    A.Normalize(mean , std),\n",
    "    A.pytorch.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handy-pencil",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x , val_x , train_y , val_y = train_test_split(df , df['label'] , test_size=0.3 , shuffle = False)\n",
    "\n",
    "train_dataset = CustomDataset(img_path , train_x , train_transforms)\n",
    "val_dataset = CustomDataset(img_path , val_x , val_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size , shuffle = True , num_workers = num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size , shuffle = True , num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "moved-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_layers = {'layer2': 1, 'layer3': 2, 'layer4': 3}\n",
    "# https://github.com/biubug6/Pytorch_Retinaface\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels , out_channels , kernel_size):\n",
    "        super(Conv, self).__init__()\n",
    "        if kernel_size == 1 :\n",
    "            self.padding = 0 \n",
    "            self.stride = 2\n",
    "        else : \n",
    "            self.padding = 1\n",
    "            self.stride = 1\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels = in_channels , \n",
    "                            out_channels = 1024,\n",
    "                            kernel_size = kernel_size,\n",
    "                            stride = self.stride ,\n",
    "                             padding = self.padding )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.ReLU = nn.ReLU(True)\n",
    "        self.fc2 = nn.Linear(128 ,self.out_channels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "class AgeHead(nn.Module):\n",
    "    def __init__(self, Conv):\n",
    "        super(AgeHead,self).__init__()\n",
    "        self.conv = Conv(in_channels = 2048 , \n",
    "                            out_channels = 3,\n",
    "                            kernel_size = 3)\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class GenderHead(nn.Module):\n",
    "    def __init__(self, Conv):\n",
    "        super(GenderHead,self).__init__()\n",
    "        self.conv = Conv(in_channels = 2048 , \n",
    "                            out_channels = 2,\n",
    "                            kernel_size = 3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class MaskHead(nn.Module):\n",
    "    def __init__(self, Conv):\n",
    "        super(MaskHead,self).__init__()\n",
    "        self.conv = Conv(in_channels = 2048 , \n",
    "                            out_channels = 3,\n",
    "                            kernel_size = 3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel , self).__init__()\n",
    "        backbone = models.resnet50(pretrained = True)\n",
    "        self.backbone = torch.nn.Sequential(*(list(backbone.children())[:-2])) # (-1, 2048,8,8)\n",
    "        self.AgeHead = AgeHead(Conv)\n",
    "        self.GenderHead = GenderHead(Conv)\n",
    "        self.MaskHead = MaskHead(Conv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        y1 = self.AgeHead(x)\n",
    "        y2 = self.GenderHead(x)\n",
    "        y3 = self.MaskHead(x)\n",
    "        out = (y1 , y2 ,y3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vital-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None , alpha = 0.25,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob * self.alpha,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "class MultiLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ground_truth(tuple) : Ground truth labels for a batch\n",
    "        shape : [batch_size , 3] -> 3 = (age , gender , mask)\n",
    "    prediction(tuple) : \n",
    "        shape : [batch_size , num_classes , 3]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma = 2 , alpha = 0.25 , lamb = 1, size_average = True ):\n",
    "        super(MultiLoss , self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.loss = FocalLoss(alpha = self.alpha , gamma = self.gamma)\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def forward(self, y_pred ,target):\n",
    "        \n",
    "        pred_age , pred_gender , pred_mask = y_pred     # (batch_size , 3) , (batch_size , 2) , (batch_size , 3)\n",
    "        target_age , target_gender , target_mask = target\n",
    "        \n",
    "        age_loss = self.loss(pred_age , target_age).to(device)\n",
    "        gender_loss = self.loss(pred_gender , target_gender).to(device)\n",
    "        mask_loss = self.loss(pred_mask , target_mask).to(device)\n",
    "     \n",
    "        total_loss =  gender_loss + mask_loss + self.lamb * age_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "naked-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs , train_loader, val_loader , model , criterion , optimizer , lr_scheduler , num_early_stop = 5):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = 0\n",
    "    best_val_loss = 999999999\n",
    "    eps  = 1e-8\n",
    "    early_stop = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if early_stop >= num_early_stop : break\n",
    "            \n",
    "        ##################################### train ################################\n",
    "        model.train()\n",
    "        \n",
    "        loss_train_sum = 0\n",
    "        acc_train_sum = 0\n",
    "        f1_train_sum = 0\n",
    "        \n",
    "        for i , (img , target) in enumerate(tqdm(train_loader)):\n",
    "            img = img.to(device)\n",
    "            target = [t.to(device) for t in target]\n",
    "\n",
    "            y_pred = model.forward(img)\n",
    "            loss = criterion(y_pred, target)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train_sum += loss\n",
    "            \n",
    "            acc_train = 0 \n",
    "            f1_train = 0\n",
    "            \n",
    "            ## Cal acc, macrof1\n",
    "            for i in range(3):\n",
    "                pred = y_pred[i].argmax(1)\n",
    "                tgt = target[i]\n",
    "                acc_train += (pred == tgt).sum().item()\n",
    "                f1_train += f1_score(tgt.data.detach().cpu(), pred.detach().cpu(), average='macro')\n",
    "                \n",
    "            acc_train_sum += (acc_train / 3)\n",
    "            f1_train_sum += (f1_train / 3)\n",
    "        \n",
    "        loss_train_avg = loss_train_sum / len(train_loader)\n",
    "        acc_train_avg = acc_train_sum / len(train_loader)\n",
    "        f1_train_avg = f1_train_sum /  len(train_loader)\n",
    "        print(f\" epoch:[{epoch+1}/{epochs}] cost:[{loss_train_avg:.3f}] acc : [{acc_train_avg : .3f}]  f1: [{f1_train_avg : .3f}]\" )\n",
    "        print(f\" learning rate : {lr_scheduler.get_last_lr()}\")\n",
    "        \n",
    "        ##################################### eval ################################\n",
    "        model.eval()\n",
    "        \n",
    "        loss_val_sum = 0\n",
    "        acc_val_sum = 0\n",
    "        f1_val_sum = 0\n",
    "        \n",
    "        for i , (img , target) in enumerate(tqdm(val_loader)):\n",
    "            img = img.to(device)\n",
    "            target = [t.to(device) for t in target]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = model.forward(img)\n",
    "                loss = criterion(y_pred, target)\n",
    "            \n",
    "            loss_val_sum += loss\n",
    "            \n",
    "            acc_val = 0 \n",
    "            f1_val = 0\n",
    "            \n",
    "            ## Cal acc, macrof1\n",
    "            for i in range(3):\n",
    "                pred = y_pred[i].argmax(1)\n",
    "                tgt = target[i]\n",
    "                acc_val += (pred == tgt).sum().item()\n",
    "                f1_val += f1_score(tgt.data.detach().cpu(), pred.detach().cpu(), average='macro')\n",
    "                \n",
    "            acc_val_sum += (acc_val / 3)\n",
    "            f1_val_sum += (f1_val / 3)\n",
    "                    \n",
    "        \n",
    "        loss_val_avg = loss_val_sum / len(val_loader)\n",
    "        acc_val_avg = acc_val_sum / len(val_loader)\n",
    "        f1_val_avg = f1_val_sum /  len(val_loader)\n",
    "        \n",
    "        print(f\" epoch:[{epoch+1}/{epochs}] cost:[{loss_val_avg:.3f}] acc : [{acc_val_avg : .3f}] f1: [{f1_val_avg : .3f}]\")\n",
    "        \n",
    "        if lr_scheduler : \n",
    "            lr_scheduler.step()\n",
    "        \n",
    " \n",
    "        if best_val_loss > loss_val_avg:\n",
    "            best_val_loss = loss_val_avg\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            early_stop = 0\n",
    "        \n",
    "        else :\n",
    "            early_stop += 1\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), f'NewModel_add_fc_base_{best_epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demographic-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = BaseModel().to(device)\n",
    "criterion =  MultiLoss(gamma = 2 , alpha = 0.25 , lamb = 2)\n",
    "optimizer = optim.AdamW([\n",
    "        {'params': Model.backbone.parameters(), 'lr': 3e-4}\n",
    "    ] , lr = learning_rate , weight_decay=0.005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5, T_mult = 2, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eligible-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:27<00:00,  2.37it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[1/100] cost:[0.052] acc : [ 60.432]  f1: [ 0.909]\n",
      " learning rate : [0.0003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.59it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[1/100] cost:[0.092] acc : [ 58.120] f1: [ 0.822]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:27<00:00,  2.38it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[2/100] cost:[0.019] acc : [ 62.680]  f1: [ 0.966]\n",
      " learning rate : [0.0002714480406590546]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.47it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[2/100] cost:[0.103] acc : [ 59.772] f1: [ 0.860]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:27<00:00,  2.38it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[3/100] cost:[0.009] acc : [ 63.317]  f1: [ 0.983]\n",
      " learning rate : [0.0001966980406590546]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.52it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[3/100] cost:[0.108] acc : [ 59.258] f1: [ 0.843]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:26<00:00,  2.38it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[4/100] cost:[0.003] acc : [ 63.683]  f1: [ 0.993]\n",
      " learning rate : [0.00010430195934094535]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.57it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[4/100] cost:[0.081] acc : [ 60.554] f1: [ 0.882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:27<00:00,  2.38it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[5/100] cost:[0.001] acc : [ 63.837]  f1: [ 0.998]\n",
      " learning rate : [2.955195934094537e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.49it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[5/100] cost:[0.087] acc : [ 60.476] f1: [ 0.874]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:26<00:00,  2.38it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[6/100] cost:[0.023] acc : [ 62.390]  f1: [ 0.959]\n",
      " learning rate : [0.0003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.48it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[6/100] cost:[0.120] acc : [ 59.453] f1: [ 0.834]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:27<00:00,  2.37it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[7/100] cost:[0.011] acc : [ 63.130]  f1: [ 0.979]\n",
      " learning rate : [0.0002926829491861254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:14<00:00,  6.01it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[7/100] cost:[0.084] acc : [ 60.689] f1: [ 0.873]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:27<00:00,  2.37it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[8/100] cost:[0.007] acc : [ 63.451]  f1: [ 0.987]\n",
      " learning rate : [0.0002714480406590546]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:15<00:00,  5.62it/s]\n",
      "  0%|          | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[8/100] cost:[0.109] acc : [ 59.127] f1: [ 0.848]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:27<00:00,  2.37it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[9/100] cost:[0.003] acc : [ 63.651]  f1: [ 0.993]\n",
      " learning rate : [0.0002383738952177247]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:14<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:[9/100] cost:[0.127] acc : [ 59.809] f1: [ 0.856]\n"
     ]
    }
   ],
   "source": [
    "train(epochs , train_loader , val_loader , Model, criterion , optimizer , lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "czech-accent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = BaseModel().to(device)\n",
    "Model.load_state_dict(torch.load('NewModel_add_fc_base_4.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "legendary-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "clear-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "motivated-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cal_label(pred_age , pred_gender , pred_mask):\n",
    "    \"\"\"\n",
    "    pred_age : [batch_size , 3]\n",
    "    pred_gender : [batch_size , 2]\n",
    "    pred_mask : [batch_size , 3]\n",
    "    \"\"\"\n",
    "    age = pred_age.argmax(dim=-1)\n",
    "    gender = pred_gender.argmax(dim=-1)\n",
    "    mask = pred_mask.argmax(dim=-1)\n",
    "    \n",
    "    multi_class_label = mask * 6 + gender * 3 + age\n",
    "    return multi_class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "practical-interview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "img_size = 224\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "                                    transforms.Resize((img_size, img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "Model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred_age , pred_gender , pred_mask = Model(images)\n",
    "        pred = Cal_label(pred_age , pred_gender , pred_mask)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'New_model_submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arctic-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /opt/ml/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('efficientnet_b3' , pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "previous-antarctica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 40, 128, 128]           1,080\n",
      "       BatchNorm2d-2         [-1, 40, 128, 128]              80\n",
      "           SwishMe-3         [-1, 40, 128, 128]               0\n",
      "            Conv2d-4         [-1, 40, 128, 128]             360\n",
      "       BatchNorm2d-5         [-1, 40, 128, 128]              80\n",
      "           SwishMe-6         [-1, 40, 128, 128]               0\n",
      "            Conv2d-7             [-1, 10, 1, 1]             410\n",
      "           SwishMe-8             [-1, 10, 1, 1]               0\n",
      "            Conv2d-9             [-1, 40, 1, 1]             440\n",
      "    SqueezeExcite-10         [-1, 40, 128, 128]               0\n",
      "           Conv2d-11         [-1, 24, 128, 128]             960\n",
      "      BatchNorm2d-12         [-1, 24, 128, 128]              48\n",
      "         Identity-13         [-1, 24, 128, 128]               0\n",
      "DepthwiseSeparableConv-14         [-1, 24, 128, 128]               0\n",
      "           Conv2d-15         [-1, 24, 128, 128]             216\n",
      "      BatchNorm2d-16         [-1, 24, 128, 128]              48\n",
      "          SwishMe-17         [-1, 24, 128, 128]               0\n",
      "           Conv2d-18              [-1, 6, 1, 1]             150\n",
      "          SwishMe-19              [-1, 6, 1, 1]               0\n",
      "           Conv2d-20             [-1, 24, 1, 1]             168\n",
      "    SqueezeExcite-21         [-1, 24, 128, 128]               0\n",
      "           Conv2d-22         [-1, 24, 128, 128]             576\n",
      "      BatchNorm2d-23         [-1, 24, 128, 128]              48\n",
      "         Identity-24         [-1, 24, 128, 128]               0\n",
      "DepthwiseSeparableConv-25         [-1, 24, 128, 128]               0\n",
      "           Conv2d-26        [-1, 144, 128, 128]           3,456\n",
      "      BatchNorm2d-27        [-1, 144, 128, 128]             288\n",
      "          SwishMe-28        [-1, 144, 128, 128]               0\n",
      "           Conv2d-29          [-1, 144, 64, 64]           1,296\n",
      "      BatchNorm2d-30          [-1, 144, 64, 64]             288\n",
      "          SwishMe-31          [-1, 144, 64, 64]               0\n",
      "           Conv2d-32              [-1, 6, 1, 1]             870\n",
      "          SwishMe-33              [-1, 6, 1, 1]               0\n",
      "           Conv2d-34            [-1, 144, 1, 1]           1,008\n",
      "    SqueezeExcite-35          [-1, 144, 64, 64]               0\n",
      "           Conv2d-36           [-1, 32, 64, 64]           4,608\n",
      "      BatchNorm2d-37           [-1, 32, 64, 64]              64\n",
      " InvertedResidual-38           [-1, 32, 64, 64]               0\n",
      "           Conv2d-39          [-1, 192, 64, 64]           6,144\n",
      "      BatchNorm2d-40          [-1, 192, 64, 64]             384\n",
      "          SwishMe-41          [-1, 192, 64, 64]               0\n",
      "           Conv2d-42          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-43          [-1, 192, 64, 64]             384\n",
      "          SwishMe-44          [-1, 192, 64, 64]               0\n",
      "           Conv2d-45              [-1, 8, 1, 1]           1,544\n",
      "          SwishMe-46              [-1, 8, 1, 1]               0\n",
      "           Conv2d-47            [-1, 192, 1, 1]           1,728\n",
      "    SqueezeExcite-48          [-1, 192, 64, 64]               0\n",
      "           Conv2d-49           [-1, 32, 64, 64]           6,144\n",
      "      BatchNorm2d-50           [-1, 32, 64, 64]              64\n",
      " InvertedResidual-51           [-1, 32, 64, 64]               0\n",
      "           Conv2d-52          [-1, 192, 64, 64]           6,144\n",
      "      BatchNorm2d-53          [-1, 192, 64, 64]             384\n",
      "          SwishMe-54          [-1, 192, 64, 64]               0\n",
      "           Conv2d-55          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-56          [-1, 192, 64, 64]             384\n",
      "          SwishMe-57          [-1, 192, 64, 64]               0\n",
      "           Conv2d-58              [-1, 8, 1, 1]           1,544\n",
      "          SwishMe-59              [-1, 8, 1, 1]               0\n",
      "           Conv2d-60            [-1, 192, 1, 1]           1,728\n",
      "    SqueezeExcite-61          [-1, 192, 64, 64]               0\n",
      "           Conv2d-62           [-1, 32, 64, 64]           6,144\n",
      "      BatchNorm2d-63           [-1, 32, 64, 64]              64\n",
      " InvertedResidual-64           [-1, 32, 64, 64]               0\n",
      "           Conv2d-65          [-1, 192, 64, 64]           6,144\n",
      "      BatchNorm2d-66          [-1, 192, 64, 64]             384\n",
      "          SwishMe-67          [-1, 192, 64, 64]               0\n",
      "           Conv2d-68          [-1, 192, 32, 32]           4,800\n",
      "      BatchNorm2d-69          [-1, 192, 32, 32]             384\n",
      "          SwishMe-70          [-1, 192, 32, 32]               0\n",
      "           Conv2d-71              [-1, 8, 1, 1]           1,544\n",
      "          SwishMe-72              [-1, 8, 1, 1]               0\n",
      "           Conv2d-73            [-1, 192, 1, 1]           1,728\n",
      "    SqueezeExcite-74          [-1, 192, 32, 32]               0\n",
      "           Conv2d-75           [-1, 48, 32, 32]           9,216\n",
      "      BatchNorm2d-76           [-1, 48, 32, 32]              96\n",
      " InvertedResidual-77           [-1, 48, 32, 32]               0\n",
      "           Conv2d-78          [-1, 288, 32, 32]          13,824\n",
      "      BatchNorm2d-79          [-1, 288, 32, 32]             576\n",
      "          SwishMe-80          [-1, 288, 32, 32]               0\n",
      "           Conv2d-81          [-1, 288, 32, 32]           7,200\n",
      "      BatchNorm2d-82          [-1, 288, 32, 32]             576\n",
      "          SwishMe-83          [-1, 288, 32, 32]               0\n",
      "           Conv2d-84             [-1, 12, 1, 1]           3,468\n",
      "          SwishMe-85             [-1, 12, 1, 1]               0\n",
      "           Conv2d-86            [-1, 288, 1, 1]           3,744\n",
      "    SqueezeExcite-87          [-1, 288, 32, 32]               0\n",
      "           Conv2d-88           [-1, 48, 32, 32]          13,824\n",
      "      BatchNorm2d-89           [-1, 48, 32, 32]              96\n",
      " InvertedResidual-90           [-1, 48, 32, 32]               0\n",
      "           Conv2d-91          [-1, 288, 32, 32]          13,824\n",
      "      BatchNorm2d-92          [-1, 288, 32, 32]             576\n",
      "          SwishMe-93          [-1, 288, 32, 32]               0\n",
      "           Conv2d-94          [-1, 288, 32, 32]           7,200\n",
      "      BatchNorm2d-95          [-1, 288, 32, 32]             576\n",
      "          SwishMe-96          [-1, 288, 32, 32]               0\n",
      "           Conv2d-97             [-1, 12, 1, 1]           3,468\n",
      "          SwishMe-98             [-1, 12, 1, 1]               0\n",
      "           Conv2d-99            [-1, 288, 1, 1]           3,744\n",
      "   SqueezeExcite-100          [-1, 288, 32, 32]               0\n",
      "          Conv2d-101           [-1, 48, 32, 32]          13,824\n",
      "     BatchNorm2d-102           [-1, 48, 32, 32]              96\n",
      "InvertedResidual-103           [-1, 48, 32, 32]               0\n",
      "          Conv2d-104          [-1, 288, 32, 32]          13,824\n",
      "     BatchNorm2d-105          [-1, 288, 32, 32]             576\n",
      "         SwishMe-106          [-1, 288, 32, 32]               0\n",
      "          Conv2d-107          [-1, 288, 16, 16]           2,592\n",
      "     BatchNorm2d-108          [-1, 288, 16, 16]             576\n",
      "         SwishMe-109          [-1, 288, 16, 16]               0\n",
      "          Conv2d-110             [-1, 12, 1, 1]           3,468\n",
      "         SwishMe-111             [-1, 12, 1, 1]               0\n",
      "          Conv2d-112            [-1, 288, 1, 1]           3,744\n",
      "   SqueezeExcite-113          [-1, 288, 16, 16]               0\n",
      "          Conv2d-114           [-1, 96, 16, 16]          27,648\n",
      "     BatchNorm2d-115           [-1, 96, 16, 16]             192\n",
      "InvertedResidual-116           [-1, 96, 16, 16]               0\n",
      "          Conv2d-117          [-1, 576, 16, 16]          55,296\n",
      "     BatchNorm2d-118          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-119          [-1, 576, 16, 16]               0\n",
      "          Conv2d-120          [-1, 576, 16, 16]           5,184\n",
      "     BatchNorm2d-121          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-122          [-1, 576, 16, 16]               0\n",
      "          Conv2d-123             [-1, 24, 1, 1]          13,848\n",
      "         SwishMe-124             [-1, 24, 1, 1]               0\n",
      "          Conv2d-125            [-1, 576, 1, 1]          14,400\n",
      "   SqueezeExcite-126          [-1, 576, 16, 16]               0\n",
      "          Conv2d-127           [-1, 96, 16, 16]          55,296\n",
      "     BatchNorm2d-128           [-1, 96, 16, 16]             192\n",
      "InvertedResidual-129           [-1, 96, 16, 16]               0\n",
      "          Conv2d-130          [-1, 576, 16, 16]          55,296\n",
      "     BatchNorm2d-131          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-132          [-1, 576, 16, 16]               0\n",
      "          Conv2d-133          [-1, 576, 16, 16]           5,184\n",
      "     BatchNorm2d-134          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-135          [-1, 576, 16, 16]               0\n",
      "          Conv2d-136             [-1, 24, 1, 1]          13,848\n",
      "         SwishMe-137             [-1, 24, 1, 1]               0\n",
      "          Conv2d-138            [-1, 576, 1, 1]          14,400\n",
      "   SqueezeExcite-139          [-1, 576, 16, 16]               0\n",
      "          Conv2d-140           [-1, 96, 16, 16]          55,296\n",
      "     BatchNorm2d-141           [-1, 96, 16, 16]             192\n",
      "InvertedResidual-142           [-1, 96, 16, 16]               0\n",
      "          Conv2d-143          [-1, 576, 16, 16]          55,296\n",
      "     BatchNorm2d-144          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-145          [-1, 576, 16, 16]               0\n",
      "          Conv2d-146          [-1, 576, 16, 16]           5,184\n",
      "     BatchNorm2d-147          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-148          [-1, 576, 16, 16]               0\n",
      "          Conv2d-149             [-1, 24, 1, 1]          13,848\n",
      "         SwishMe-150             [-1, 24, 1, 1]               0\n",
      "          Conv2d-151            [-1, 576, 1, 1]          14,400\n",
      "   SqueezeExcite-152          [-1, 576, 16, 16]               0\n",
      "          Conv2d-153           [-1, 96, 16, 16]          55,296\n",
      "     BatchNorm2d-154           [-1, 96, 16, 16]             192\n",
      "InvertedResidual-155           [-1, 96, 16, 16]               0\n",
      "          Conv2d-156          [-1, 576, 16, 16]          55,296\n",
      "     BatchNorm2d-157          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-158          [-1, 576, 16, 16]               0\n",
      "          Conv2d-159          [-1, 576, 16, 16]           5,184\n",
      "     BatchNorm2d-160          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-161          [-1, 576, 16, 16]               0\n",
      "          Conv2d-162             [-1, 24, 1, 1]          13,848\n",
      "         SwishMe-163             [-1, 24, 1, 1]               0\n",
      "          Conv2d-164            [-1, 576, 1, 1]          14,400\n",
      "   SqueezeExcite-165          [-1, 576, 16, 16]               0\n",
      "          Conv2d-166           [-1, 96, 16, 16]          55,296\n",
      "     BatchNorm2d-167           [-1, 96, 16, 16]             192\n",
      "InvertedResidual-168           [-1, 96, 16, 16]               0\n",
      "          Conv2d-169          [-1, 576, 16, 16]          55,296\n",
      "     BatchNorm2d-170          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-171          [-1, 576, 16, 16]               0\n",
      "          Conv2d-172          [-1, 576, 16, 16]          14,400\n",
      "     BatchNorm2d-173          [-1, 576, 16, 16]           1,152\n",
      "         SwishMe-174          [-1, 576, 16, 16]               0\n",
      "          Conv2d-175             [-1, 24, 1, 1]          13,848\n",
      "         SwishMe-176             [-1, 24, 1, 1]               0\n",
      "          Conv2d-177            [-1, 576, 1, 1]          14,400\n",
      "   SqueezeExcite-178          [-1, 576, 16, 16]               0\n",
      "          Conv2d-179          [-1, 136, 16, 16]          78,336\n",
      "     BatchNorm2d-180          [-1, 136, 16, 16]             272\n",
      "InvertedResidual-181          [-1, 136, 16, 16]               0\n",
      "          Conv2d-182          [-1, 816, 16, 16]         110,976\n",
      "     BatchNorm2d-183          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-184          [-1, 816, 16, 16]               0\n",
      "          Conv2d-185          [-1, 816, 16, 16]          20,400\n",
      "     BatchNorm2d-186          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-187          [-1, 816, 16, 16]               0\n",
      "          Conv2d-188             [-1, 34, 1, 1]          27,778\n",
      "         SwishMe-189             [-1, 34, 1, 1]               0\n",
      "          Conv2d-190            [-1, 816, 1, 1]          28,560\n",
      "   SqueezeExcite-191          [-1, 816, 16, 16]               0\n",
      "          Conv2d-192          [-1, 136, 16, 16]         110,976\n",
      "     BatchNorm2d-193          [-1, 136, 16, 16]             272\n",
      "InvertedResidual-194          [-1, 136, 16, 16]               0\n",
      "          Conv2d-195          [-1, 816, 16, 16]         110,976\n",
      "     BatchNorm2d-196          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-197          [-1, 816, 16, 16]               0\n",
      "          Conv2d-198          [-1, 816, 16, 16]          20,400\n",
      "     BatchNorm2d-199          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-200          [-1, 816, 16, 16]               0\n",
      "          Conv2d-201             [-1, 34, 1, 1]          27,778\n",
      "         SwishMe-202             [-1, 34, 1, 1]               0\n",
      "          Conv2d-203            [-1, 816, 1, 1]          28,560\n",
      "   SqueezeExcite-204          [-1, 816, 16, 16]               0\n",
      "          Conv2d-205          [-1, 136, 16, 16]         110,976\n",
      "     BatchNorm2d-206          [-1, 136, 16, 16]             272\n",
      "InvertedResidual-207          [-1, 136, 16, 16]               0\n",
      "          Conv2d-208          [-1, 816, 16, 16]         110,976\n",
      "     BatchNorm2d-209          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-210          [-1, 816, 16, 16]               0\n",
      "          Conv2d-211          [-1, 816, 16, 16]          20,400\n",
      "     BatchNorm2d-212          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-213          [-1, 816, 16, 16]               0\n",
      "          Conv2d-214             [-1, 34, 1, 1]          27,778\n",
      "         SwishMe-215             [-1, 34, 1, 1]               0\n",
      "          Conv2d-216            [-1, 816, 1, 1]          28,560\n",
      "   SqueezeExcite-217          [-1, 816, 16, 16]               0\n",
      "          Conv2d-218          [-1, 136, 16, 16]         110,976\n",
      "     BatchNorm2d-219          [-1, 136, 16, 16]             272\n",
      "InvertedResidual-220          [-1, 136, 16, 16]               0\n",
      "          Conv2d-221          [-1, 816, 16, 16]         110,976\n",
      "     BatchNorm2d-222          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-223          [-1, 816, 16, 16]               0\n",
      "          Conv2d-224          [-1, 816, 16, 16]          20,400\n",
      "     BatchNorm2d-225          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-226          [-1, 816, 16, 16]               0\n",
      "          Conv2d-227             [-1, 34, 1, 1]          27,778\n",
      "         SwishMe-228             [-1, 34, 1, 1]               0\n",
      "          Conv2d-229            [-1, 816, 1, 1]          28,560\n",
      "   SqueezeExcite-230          [-1, 816, 16, 16]               0\n",
      "          Conv2d-231          [-1, 136, 16, 16]         110,976\n",
      "     BatchNorm2d-232          [-1, 136, 16, 16]             272\n",
      "InvertedResidual-233          [-1, 136, 16, 16]               0\n",
      "          Conv2d-234          [-1, 816, 16, 16]         110,976\n",
      "     BatchNorm2d-235          [-1, 816, 16, 16]           1,632\n",
      "         SwishMe-236          [-1, 816, 16, 16]               0\n",
      "          Conv2d-237            [-1, 816, 8, 8]          20,400\n",
      "     BatchNorm2d-238            [-1, 816, 8, 8]           1,632\n",
      "         SwishMe-239            [-1, 816, 8, 8]               0\n",
      "          Conv2d-240             [-1, 34, 1, 1]          27,778\n",
      "         SwishMe-241             [-1, 34, 1, 1]               0\n",
      "          Conv2d-242            [-1, 816, 1, 1]          28,560\n",
      "   SqueezeExcite-243            [-1, 816, 8, 8]               0\n",
      "          Conv2d-244            [-1, 232, 8, 8]         189,312\n",
      "     BatchNorm2d-245            [-1, 232, 8, 8]             464\n",
      "InvertedResidual-246            [-1, 232, 8, 8]               0\n",
      "          Conv2d-247           [-1, 1392, 8, 8]         322,944\n",
      "     BatchNorm2d-248           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-249           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-250           [-1, 1392, 8, 8]          34,800\n",
      "     BatchNorm2d-251           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-252           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-253             [-1, 58, 1, 1]          80,794\n",
      "         SwishMe-254             [-1, 58, 1, 1]               0\n",
      "          Conv2d-255           [-1, 1392, 1, 1]          82,128\n",
      "   SqueezeExcite-256           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-257            [-1, 232, 8, 8]         322,944\n",
      "     BatchNorm2d-258            [-1, 232, 8, 8]             464\n",
      "InvertedResidual-259            [-1, 232, 8, 8]               0\n",
      "          Conv2d-260           [-1, 1392, 8, 8]         322,944\n",
      "     BatchNorm2d-261           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-262           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-263           [-1, 1392, 8, 8]          34,800\n",
      "     BatchNorm2d-264           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-265           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-266             [-1, 58, 1, 1]          80,794\n",
      "         SwishMe-267             [-1, 58, 1, 1]               0\n",
      "          Conv2d-268           [-1, 1392, 1, 1]          82,128\n",
      "   SqueezeExcite-269           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-270            [-1, 232, 8, 8]         322,944\n",
      "     BatchNorm2d-271            [-1, 232, 8, 8]             464\n",
      "InvertedResidual-272            [-1, 232, 8, 8]               0\n",
      "          Conv2d-273           [-1, 1392, 8, 8]         322,944\n",
      "     BatchNorm2d-274           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-275           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-276           [-1, 1392, 8, 8]          34,800\n",
      "     BatchNorm2d-277           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-278           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-279             [-1, 58, 1, 1]          80,794\n",
      "         SwishMe-280             [-1, 58, 1, 1]               0\n",
      "          Conv2d-281           [-1, 1392, 1, 1]          82,128\n",
      "   SqueezeExcite-282           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-283            [-1, 232, 8, 8]         322,944\n",
      "     BatchNorm2d-284            [-1, 232, 8, 8]             464\n",
      "InvertedResidual-285            [-1, 232, 8, 8]               0\n",
      "          Conv2d-286           [-1, 1392, 8, 8]         322,944\n",
      "     BatchNorm2d-287           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-288           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-289           [-1, 1392, 8, 8]          34,800\n",
      "     BatchNorm2d-290           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-291           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-292             [-1, 58, 1, 1]          80,794\n",
      "         SwishMe-293             [-1, 58, 1, 1]               0\n",
      "          Conv2d-294           [-1, 1392, 1, 1]          82,128\n",
      "   SqueezeExcite-295           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-296            [-1, 232, 8, 8]         322,944\n",
      "     BatchNorm2d-297            [-1, 232, 8, 8]             464\n",
      "InvertedResidual-298            [-1, 232, 8, 8]               0\n",
      "          Conv2d-299           [-1, 1392, 8, 8]         322,944\n",
      "     BatchNorm2d-300           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-301           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-302           [-1, 1392, 8, 8]          34,800\n",
      "     BatchNorm2d-303           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-304           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-305             [-1, 58, 1, 1]          80,794\n",
      "         SwishMe-306             [-1, 58, 1, 1]               0\n",
      "          Conv2d-307           [-1, 1392, 1, 1]          82,128\n",
      "   SqueezeExcite-308           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-309            [-1, 232, 8, 8]         322,944\n",
      "     BatchNorm2d-310            [-1, 232, 8, 8]             464\n",
      "InvertedResidual-311            [-1, 232, 8, 8]               0\n",
      "          Conv2d-312           [-1, 1392, 8, 8]         322,944\n",
      "     BatchNorm2d-313           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-314           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-315           [-1, 1392, 8, 8]          12,528\n",
      "     BatchNorm2d-316           [-1, 1392, 8, 8]           2,784\n",
      "         SwishMe-317           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-318             [-1, 58, 1, 1]          80,794\n",
      "         SwishMe-319             [-1, 58, 1, 1]               0\n",
      "          Conv2d-320           [-1, 1392, 1, 1]          82,128\n",
      "   SqueezeExcite-321           [-1, 1392, 8, 8]               0\n",
      "          Conv2d-322            [-1, 384, 8, 8]         534,528\n",
      "     BatchNorm2d-323            [-1, 384, 8, 8]             768\n",
      "InvertedResidual-324            [-1, 384, 8, 8]               0\n",
      "          Conv2d-325           [-1, 2304, 8, 8]         884,736\n",
      "     BatchNorm2d-326           [-1, 2304, 8, 8]           4,608\n",
      "         SwishMe-327           [-1, 2304, 8, 8]               0\n",
      "          Conv2d-328           [-1, 2304, 8, 8]          20,736\n",
      "     BatchNorm2d-329           [-1, 2304, 8, 8]           4,608\n",
      "         SwishMe-330           [-1, 2304, 8, 8]               0\n",
      "          Conv2d-331             [-1, 96, 1, 1]         221,280\n",
      "         SwishMe-332             [-1, 96, 1, 1]               0\n",
      "          Conv2d-333           [-1, 2304, 1, 1]         223,488\n",
      "   SqueezeExcite-334           [-1, 2304, 8, 8]               0\n",
      "          Conv2d-335            [-1, 384, 8, 8]         884,736\n",
      "     BatchNorm2d-336            [-1, 384, 8, 8]             768\n",
      "InvertedResidual-337            [-1, 384, 8, 8]               0\n",
      "          Conv2d-338           [-1, 1536, 8, 8]         589,824\n",
      "     BatchNorm2d-339           [-1, 1536, 8, 8]           3,072\n",
      "         SwishMe-340           [-1, 1536, 8, 8]               0\n",
      "AdaptiveAvgPool2d-341           [-1, 1536, 1, 1]               0\n",
      "SelectAdaptivePool2d-342                 [-1, 1536]               0\n",
      "          Linear-343                 [-1, 1000]       1,537,000\n",
      "================================================================\n",
      "Total params: 12,233,232\n",
      "Trainable params: 12,233,232\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 443.07\n",
      "Params size (MB): 46.67\n",
      "Estimated Total Size (MB): 490.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model.cuda(), (3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "numerical-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import timm\n",
    "from timm.models.layers.classifier import ClassifierHead\n",
    "import torch.nn.utils.weight_norm as weightNorm\n",
    "\n",
    "class MultiBranchModel(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    MultiBranchModel \n",
    "        - output (tuppe):([batch_size , 3] ,[batch_size , 2] , [batch_size , 3] )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, dropout_num = 4 ,dropout_p = 0.5):\n",
    "        super().__init__()\n",
    "        model = timm.create_model('efficientnet_b0' , pretrained = True)\n",
    "        self.backbone = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        self.dropout_num = dropout_num\n",
    "        self.maskdropouts = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(dropout_num)])\n",
    "        self.maskfc = nn.Linear(1280 , num_classes)\n",
    "        \n",
    "        nn.init.normal_(self.maskfc.weight, std=0.001)\n",
    "        nn.init.constant_(self.maskfc.bias, 0) \n",
    "        \n",
    "        self.genderdropouts = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(dropout_num)])\n",
    "        self.genderfc = nn.Linear(1280 , num_classes-1)\n",
    "        nn.init.normal_(self.genderfc.weight, std=0.001)\n",
    "        nn.init.constant_(self.genderfc.bias, 0)   \n",
    "        \n",
    "        self.agedropouts = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(dropout_num)])\n",
    "        self.agefc = nn.Linear(1280 , num_classes)\n",
    "        nn.init.normal_(self.agefc.weight, std=0.001)\n",
    "        nn.init.constant_(self.agefc.bias, 0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        # Mask\n",
    "        for i,dropout in enumerate(self.maskdropouts):\n",
    "            if i== 0:\n",
    "                mask = self.maskfc(dropout(x))\n",
    "            else : \n",
    "                mask += self.maskfc(dropout(x)) \n",
    "        mask /= self.dropout_num\n",
    "        \n",
    "        # Gender\n",
    "        for i,dropout in enumerate(self.genderdropouts):\n",
    "            if i== 0:\n",
    "                gender = self.genderfc(dropout(x))\n",
    "            else : \n",
    "                gender += self.genderfc(dropout(x))         \n",
    "        gender /= self.dropout_num\n",
    "        \n",
    "        # Age\n",
    "        for i,dropout in enumerate(self.agedropouts):\n",
    "            if i== 0:\n",
    "                age = self.agefc(dropout(x))\n",
    "            else : \n",
    "                age += self.agefc(dropout(x))\n",
    "        age /= self.dropout_num\n",
    "        \n",
    "        return (mask, gender, age) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "elect-failing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MultiBranchModel(3)\n",
    "for name, param in model.named_parameters():\n",
    "    if name in [ 'maskfc.weight' , 'maskfc.bias' ,'genderfc.weight' ,'genderfc.bias']:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "interim-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maskfc.weight\n",
      "maskfc.bias\n",
      "genderfc.weight\n",
      "genderfc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad :\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "smart-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param in [ 'maskfc.weight' , 'maskfc.bias' ,'genderfc.weight' ,'genderfc']:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bulgarian-priority",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute 'param'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ec4381a5c616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Parameter' object has no attribute 'param'"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    if param.param.requires_grad == False :\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "possible-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('efficientnet_b3' , pretrained = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}